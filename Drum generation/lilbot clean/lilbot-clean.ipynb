{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.utils import get_file\nimport numpy as np\nimport random\nimport sys\nimport os\nimport pdb\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Variabili che devono essere uguali in training e prediction\nn_epochs = 15 #mattia ha messo 15\nmaxlenUSER = 128\nbatchSizeSET = 128","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Two different models, just run one of them"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(maxlen, num_chars, num_layers, num_units):\n    print('Build model...')\n    model = Sequential()\n    for layer_idx in range(num_layers):\n        if layer_idx == 0:\n            #the size is maxlen x numchars. Maxlen is because of the length of a sentence (128 words) while num_chars=119\n            #which are the unique words in my vocabulary.\n            #NB: the input is one hot encoded over the vocabulary size (num_chars)\n            model.add(LSTM(num_units, return_sequences=True, input_shape=(maxlen, num_chars)))\n        else:\n            model.add(LSTM(num_units, return_sequences=False))\n        model.add(Dropout(0.2))\n\n    model.add(Dense(num_chars))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(maxlen, num_chars, num_layers, num_units):\n    model = Sequential()\n    model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, num_chars)))\n    model.add(Dropout(0.2))\n    model.add(LSTM(512, return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(LSTM(512, return_sequences=False))\n    model.add(Dropout(0.2))\n    model.add(Dense(num_chars))\n    model.add(Activation('softmax'))\n \n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Altre funzioni"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sample(preds, temperature=1.0):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Funzione principale run():"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(is_character=False, maxlen=None, num_units=None, model_prefix=''):\n\n    character_mode = is_character\n\n    if character_mode:\n        if maxlen == None:\n            maxlen = 1024\n        if num_units == None:\n            num_units = 32\n        step = 2*17 # step to create training data for truncated-BPTT\n    else: # word mode\n        if maxlen == None:\n            maxlen = 256 # maxlength used in RNN input\n        if num_units == None: \n            num_units = 512 #number of unit per layer LSTM 512 \n        step = 8\n\n    if character_mode:\n        num_char_pred = maxlen*3/2\n    else: \n        num_char_pred = 17*30 #this should be the number of elements predicted in the output. How \"long\" is my output sequence\n\n    num_layers = 2\n    # \n    if character_mode:\n        prefix = 'char'\n    else:\n        prefix = 'word'\n\n    path = '../input/dasetartificial/datasetartificial 16note.txt' # Corpus file\n    text = open(path).read()\n    print('corpus length:', len(text))\n\n    if character_mode:\n        chars = set(text)\n    else:\n        chord_seq = text.split(' ')\n        chars = set(chord_seq) #contains the unique words in my dictionary. They are 119\n        text = chord_seq #contains the full text in an array format. Each entry of my array is a word of type 0xb0110101010 \n\n    char_indices = dict((c, i) for i, c in enumerate(chars))\n    indices_char = dict((i, c) for i, c in enumerate(chars))\n    num_chars = len(char_indices) #number of unique words in my training set\n    print('total chars:', num_chars)\n\n    # cut the text in semi-redundant sequences of maxlen characters\n\n    sentences = []\n    next_chars = []\n    #Here im creating the inputs and targets for my RNN. Each single input has length maxlen.\n    #Inputs are semi-redundant, in the sense that i take a length of maxlen=128 and the step is 8. So my first part of the input\n    #will be the same and the last 8 elements are \"new\". I'm just \"slitting\" of 8 notes ahead\n    for i in range(0, len(text) - maxlen, step): #iterates over the range with steps of 8.\n        sentences.append(text[i: i + maxlen])\n        next_chars.append(text[i + maxlen])\n    print('nb sequences:', len(sentences))\n    print('Vectorization...')\n    \n    #Here i'm creating the input dataset and target dataset for my network\n    #X is a tri-dimensional vector: 1 dimension -> Sentences, 2 dimension -> Single sentence, 3 dimension -> one hot encoded vector of the single word\n    #So basically i have a structure where i have N sentences of maxlen Words where each word is represented as a one hot vector of length num_chars\n    X = np.zeros((len(sentences), maxlen, num_chars), dtype=np.bool) #Input matrix\n    y = np.zeros((len(sentences), num_chars), dtype=np.bool) #Target Matrix\n    #Here i'm actually \"populating\" the matrixes, which were initialized with all zeros\n    print('Entering initialization cycle')\n\n    for i, sentence in enumerate(sentences):\n        for t, char in enumerate(sentence):\n            X[i, t, char_indices[char]] = 1 #NB: char in this case means a whole word like oxb01011101. With char_indices[char] i'm retrieving the index of my word inside my dictionary of (words,index)\n        #print('Finished input initialization')\n        y[i, char_indices[next_chars[i]]] = 1\n    print('Completed Vectorization')\n    # build the model: 2 stacked LSTM\n    model = get_model(maxlen, num_chars, num_layers, num_units) \n    print('printing dataset')\n    print(X)\n    #Just some string declarations for folders management and names.\n    #NB: CHANGE THE / with \\ for windows! \n    result_directory = 'r_%s_%s_%d_%d_units' % (prefix, model_prefix, maxlen, num_units)\n    filepath_model = os.path.join(result_directory, '/kaggle/working/best_model.hdf')\n\n    #filepath_model = '%sbest_model.hdf' % result_directory\n    description_model = '%s, %d layers, %d units, %d maxlen, %d steps' % (prefix, num_layers, num_units, maxlen, step)\n    \n    #Usual Model checkpoints and Early Stopping\n    checker = tf.keras.callbacks.ModelCheckpoint(filepath_model, monitor='loss', verbose=0, save_best_only=True, mode='auto')\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15, verbose=0, mode='auto')\n    \n    #create a result directory if it doesn't exist\n    if not os.path.exists(result_directory):\n        os.mkdir(result_directory)\n\n    # write a description file.\n    #creates an empty file with the drscription of my model as title\n    #with open(result_directory+description_model, 'w') as f_description:\n        #pass\n\n    # train the model, output generated text after each iteration\n    batch_size = batchSizeSET #Size of a training batch. So basically i'll update my loss function every 128 input sentences (usual batch gradient descent)\n    loss_history = []\n    pt_x = [n_epochs] #this is important - default is 15\n    #An epoch is a complete iteration over the whole input training set. So 10 epochs means that i iterates 10 times over my input dataset\n    nb_epochs = [np.sum(pt_x[:i+1]) for i in range(len(pt_x))] #array containing many epochs length. The model will be fitted many times, one for each nb_epochs.\n\n    # not random seed, but the same seed for all.\n    #A random seed (or seed state, or just seed) is a number (or vector) used to initialize a pseudorandom number generator.\n    start_index = random.randint(0, len(text) - maxlen - 1)\n\n    for iteration, nb_epoch in zip(pt_x,nb_epochs):\n        if os.path.exists('/kaggle/working/stop_asap.keunwoo'):\n            os.remove('/kaggle/working/stop_asap.keunwoo')\n            break\n\n        print('-' * 50)\n        print('Iteration', iteration)\n        print(batch_size)\n        print(len(X))\n        #fitting model over nb_epochs\n        result = model.fit(X, y, batch_size=batch_size, epochs=nb_epoch, callbacks=[checker, early_stop]) \n        loss_history = loss_history + result.history['loss']\n\n        print ('Saving model after %d epochs...' % nb_epoch)\n        #Saving model weights. Saving a model trained over nb_epochs\n        model.save_weights(\"/kaggle/working/model_weights.h5\", overwrite=True) \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = run()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"/kaggle/working/model_alt.h5\", overwrite=True)\nprint(\"Saved model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import completo, Da SALTARE tendenzialmente\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.utils import get_file\nfrom keras.models import load_model\nimport numpy as np\nimport random\nimport sys\nimport os\nimport pdb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Se definita in fase di training si può saltare\ndef sample(preds, temperature=1.0):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Carica direttamente il modello."},{"metadata":{"trusted":true},"cell_type":"code","source":"def run(is_character=False, maxlen=None, num_units=None, model_prefix=''):\n    #carica modello e pesi\n    model = load_model('/kaggle/working/model_alt.h5')\n\n    character_mode = is_character\n\n    if character_mode:\n        if maxlen == None:\n            maxlen = 1024\n        if num_units == None:\n            num_units = 32\n        step = 2*17 # step to create training data for truncated-BPTT\n    else: # word mode\n        if maxlen == None:\n            maxlen = 256 # maxlength used in RNN input\n        if num_units == None: \n            num_units = 512 #number of unit per layer LSTM 512 \n        step = 8\n\n    if character_mode:\n        num_char_pred = maxlen*3/2\n    else: \n        num_char_pred = 17*30 #this should be the number of elements predicted in the output. How \"long\" is my output sequence\n\n    num_layers = 2\n    # \n    if character_mode:\n        prefix = 'char'\n    else:\n        prefix = 'word'\n\n    path = '../input/dasetartificial/datasetartificial 16note.txt' # Corpus file\n    text = open(path).read()\n    print('corpus length:', len(text))\n\n    if character_mode:\n        chars = set(text)\n    else:\n        chord_seq = text.split(' ')\n        chars = set(chord_seq) #contains the unique words in my dictionary. They are 119\n        text = chord_seq #contains the full text in an array format. Each entry of my array is a word of type 0xb0110101010 \n\n    char_indices = dict((c, i) for i, c in enumerate(chars))\n    indices_char = dict((i, c) for i, c in enumerate(chars))\n    num_chars = len(char_indices) #number of unique words in my training set\n    print('total chars:', num_chars)\n\n    # cut the text in semi-redundant sequences of maxlen characters\n\n    sentences = []\n    next_chars = []\n    #Here im creating the inputs and targets for my RNN. Each single input has length maxlen.\n    #Inputs are semi-redundant, in the sense that i take a length of maxlen=128 and the step is 8. So my first part of the input\n    #will be the same and the last 8 elements are \"new\". I'm just \"slitting\" of 8 notes ahead\n    for i in range(0, len(text) - maxlen, step): #iterates over the range with steps of 8.\n        sentences.append(text[i: i + maxlen])\n        next_chars.append(text[i + maxlen])\n    print('nb sequences:', len(sentences))\n    print('Vectorization...')\n    \n    #Here i'm creating the input dataset and target dataset for my network\n    #X is a tri-dimensional vector: 1 dimension -> Sentences, 2 dimension -> Single sentence, 3 dimension -> one hot encoded vector of the single word\n    #So basically i have a structure where i have N sentences of maxlen Words where each word is represented as a one hot vector of length num_chars\n    X = np.zeros((len(sentences), maxlen, num_chars), dtype=np.bool) #Input matrix\n    y = np.zeros((len(sentences), num_chars), dtype=np.bool) #Target Matrix\n    #Here i'm actually \"populating\" the matrixes, which were initialized with all zeros\n    print('Entering initialization cycle')\n\n    for i, sentence in enumerate(sentences):\n        for t, char in enumerate(sentence):\n            X[i, t, char_indices[char]] = 1 #NB: char in this case means a whole word like oxb01011101. With char_indices[char] i'm retrieving the index of my word inside my dictionary of (words,index)\n        #print('Finished input initialization')\n        y[i, char_indices[next_chars[i]]] = 1\n    print('Completed Vectorization')\n    \n    # build the model: 2 stacked LSTM\n    # model = get_model(maxlen, num_chars, num_layers, num_units) \n    \n    #Just some string declarations for folders management and names.\n    #NB: CHANGE THE / with \\ for windows! \n    result_directory = 'r_%s_%s_%d_%d_units' % (prefix, model_prefix, maxlen, num_units)\n    filepath_model = os.path.join(result_directory, '/kaggle/working/best_model.hdf')\n\n    #filepath_model = '%sbest_model.hdf' % result_directory\n    description_model = '%s, %d layers, %d units, %d maxlen, %d steps' % (prefix, num_layers, num_units, maxlen, step)\n    \n    #Usual Model checkpoints and Early Stopping\n    checker = tf.keras.callbacks.ModelCheckpoint(filepath_model, monitor='loss', verbose=0, save_best_only=True, mode='auto')\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15, verbose=0, mode='auto')\n    \n    #create a result directory if it doesn't exist\n    if not os.path.exists(result_directory):\n        os.mkdir(result_directory)\n\n    # write a description file.\n    #creates an empty file with the drscription of my model as title\n    with open(result_directory+description_model, 'w') as f_description:\n        pass\n\n    # train the model, output generated text after each iteration\n    batch_size = batchSizeSET #Size of a training batch. So basically i'll update my loss function every 128 input sentences (usual batch gradient descent)\n    loss_history = []\n    pt_x = [n_epochs]\n    #An epoch is a complete iteration over the whole input training set. So 10 epochs means that i iterates 10 times over my input dataset\n    nb_epochs = [np.sum(pt_x[:i+1]) for i in range(len(pt_x))] #array containing many epochs length. The model will be fitted many times, one for each nb_epochs.\n\n    # not random seed, but the same seed for all.\n    #A random seed (or seed state, or just seed) is a number (or vector) used to initialize a pseudorandom number generator.\n    start_index = random.randint(0, len(text) - maxlen - 1)\n\n    for iteration, nb_epoch in zip(pt_x,nb_epochs):\n        if os.path.exists('/kaggle/working/stop_asap.keunwoo'):\n            os.remove('/kaggle/working/stop_asap.keunwoo')\n            break\n\n        print('-' * 50)\n        print('Iteration', iteration)\n        \n        #fitting model over nb_epochs\n        #result = model.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, callbacks=[checker, early_stop]) \n        #loss_history = loss_history + result.history['loss']\n\n        print ('Saving model after %d epochs...' % nb_epoch)\n        #Saving model weights. Saving a model trained over nb_epochs\n        #model.save_weights('%smodel_after_%d.hdf'%(result_directory, nb_epoch), overwrite=True) \n        #model.load_weights(\"model_weights.h5\")\n        #w2 = model.get_weights()\n        #print(w2)\n\n        \n        for diversity in [0.9, 1.0, 1.2, 1.5, 2]:\n            #creates a .txt file where i will save my predictions\n            with open(('/kaggle/working/%sresult_%s_iter_%02d_diversity_%4.2f.txt' % (result_directory, prefix, iteration, diversity)), 'w') as f_write:\n\n                print()\n                print('----- diversity:', diversity)\n                #f_write.write('diversity:%4.2f\\n' % diversity)\n                if character_mode:\n                    generated = ''\n                else:\n                    generated = [] #simple initialization\n                #selects a random sentence from my input dataset.\n                sentence = text[start_index: start_index + maxlen]\n                seed_sentence = text[start_index: start_index + maxlen]\n\n                if character_mode:\n                    generated += sentence\n                else:\n                    #at first iteration i just add my input sentence in my generated element\n                    generated = generated + sentence\n\n\n                print('----- Generating with seed:')\n\n                if character_mode:\n                    print(sentence)\n                    sys.stdout.write(generated)\n                else:\n                    print(' '.join(sentence))\n\n                for i in range(num_char_pred): \n                    # if generated.endswith('_END_'):\n                    # \tbreak\n                    x = np.zeros((1, maxlen, num_chars)) #initialization of input. Matrix of maxlen words, each \n\n                    for t, char in enumerate(sentence):\n                        x[0, t, char_indices[char]] = 1. \n\n                    preds = model.predict(x, verbose=0)[0] \n                    #print('printo la prediction')\n                    #print(preds)\n                    next_index = sample(preds, diversity)\n                    next_char = indices_char[next_index]\n                    #print('printo il next char')\n                    #print(next_char)\n\n                    if character_mode:\n                        generated += next_char\n                        sentence = sentence[1:] + next_char\n                    else:\n                        generated.append(next_char)\n                        sentence = sentence[1:]\n                        sentence.append(next_char)\n                        #print('printo la sentence generata')\n                        #print(generated)\n\n                    if character_mode:\n                        sys.stdout.write(next_char)\n                    # else:\n                    # \tfor ch in next_char:\n                    # \t\tsys.stdout.write(ch)\t\n\n                    sys.stdout.flush()\n\n                if character_mode:\n                    f_write.write(seed_sentence + '\\n')\n                    f_write.write(generated)\n                else:\n                    f_write.write(' '.join(seed_sentence))\n                    \n                    f_write.write(' ' .join(generated))\n\n        np.save('/kaggle/working/%sloss_%s.npy'%(result_directory, prefix), loss_history)\n\n    print ('Done! You might want to run main_post_process.py to get midi files. ')\n    print ('You need python-midi (https://github.com/vishnubob/python-midi) to run it.')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Infine"},{"metadata":{"trusted":true},"cell_type":"code","source":"run()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Txt to MIDI conversion in the following cells"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install mydy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mydy import Events, FileIO, Containers, Constants\nimport pdb\nimport os\n\n#function to approximate te length to the next bar https://stackoverflow.com/questions/3407012/c-rounding-up-to-the-nearest-multiple-of-a-number\ndef roundup(numToRound, multiple):\n    if multiple == 0:\n        return numToRound\n    remainder = numToRound % multiple\n    if remainder==0:\n        return numToRound\n    return numToRound + multiple - remainder\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#Function responsible for converting midi notes into text. Since i have to train my network over the structure i decided\n#which is 0b0000000 for no note, 0b01000000 for kick ecc... i need to convert midi notes into this format.\n\n#The original script used for midi-text translation has been lost, must be re-implemented again\nPPQ = 480 # Pulse per quater note. Used in sequencers. Standard value\nevent_per_bar = 16 # to quantise.\nmin_ppq = PPQ / (event_per_bar/4)\n\n# ignore: 39 hand clap, 54 tambourine, 56 Cowbell, 58 Vibraslap, 60-81\n\n#the dictionary below maps values to other ones. Reduced the size of the used notes. For example\n#if i have an eletric snare or a stick snare, i just map both of them into a standard snare\n\ndrum_conversion = {35:36, # acoustic bass drum -> bass drum (36)\n                    37:38, 40:38, # 37:side stick, 38: acou snare, 40: electric snare\n                    43:41, # 41 low floor tom, 43 ghigh floor tom\n                    47:45, # 45 low tom, 47 low-mid tom\n                    50:48, # 50 high tom, 48 hi mid tom\n                    44:42, # 42 closed HH, 44 pedal HH\n                    57:49, # 57 Crash 2, 49 Crash 1\n                    59:51, 53:51, 55:51, # 59 Ride 2, 51 Ride 1, 53 Ride bell, 55 Splash\n                    52:49 # 52: China cymbal\n                    }\n\n#Used in the code to map elements, everything that has not one of the following number is discarded.\n#Basically i'm ignoring notes that are not in my dataset (for examle i'll ignore shakers ecc...)\n                # k, sn,cHH,oHH,LFtom,ltm,htm,Rde,Crash\nallowed_pitch = [36, 38, 42, 46, 41, 45, 48, 51, 49] # 46: open HH\ncymbals_pitch = [49, 51] # crash, ride\ncymbals_pitch = [] # crash, ride\n\n\n#mapping midi values into notes\npitch_to_midipitch = {36:Constants.C_3,  # for logic 'SoCal' drum mapping\n                        38:Constants.D_3, \n                        39:Constants.Eb_3,\n                        41:Constants.F_3,\n                        42:Constants.Gb_3,\n                        45:Constants.A_3,\n                        46:Constants.Bb_3,\n                        48:Constants.C_4,\n                        49:Constants.Db_4,\n                        51:Constants.Eb_4\n                        }\n#la singola nota è un elemento composto da pitch (numerico, pitch midi) e tick (modo per tenere il tempo in midi)\nclass Note:\n    def __init__(self, pitch, c_tick):\n        self.pitch = pitch\n        self.c_tick = c_tick # cumulated_tick of a midi note\n\n    def add_index(self, idx):\n        '''index --> 16-th note-based index starts from 0'''\n        self.idx = idx\n\nclass Note_List():\n    def __init__(self):\n        ''''''\n        self.notes = []\n        self.quantised = False\n        self.max_idx = None\n\n    def add_note(self, note):\n        '''note: instance of Note class'''\n        self.notes.append(note)\n\n    def quantise(self, minimum_ppq):\n        '''\n        e.g. if minimum_ppq=120, quantise by 16-th note.\n        \n        '''\n        if not self.quantised:\n            for note in self.notes:\n                note.c_tick = ((note.c_tick+minimum_ppq/2)//minimum_ppq)* minimum_ppq # quantise\n                #here the index is calculated. The index is an absolute index over the 16th notes.\n                #for example an index of value 34, means that my current note appears after 34 chromes\n                #it's simply calculated by dividing the cumulated tick of the note by the ticks contained in a 16th note\n                note.add_index(note.c_tick/minimum_ppq)\n            #NB: THE QUANTIZATION FUNCTION ITERATES OVER ALL THE NOTES. So first i add all the notes, then i iterate and quantize\n\n            #Does this automatically reference to the last item added?\n            #YES. The counter note will store the last element of the iteration. So basically here i'm assigning as max index the index of the last added note\n            self.max_idx = note.idx\n\n            #Here checks if if my ending is a full musical bar. For example, if my file ends with a single kick, i'll add that note.\n            #but that kick will (probably) be at the beginning of the last musical bar. So i have to \"pad\" until the end.\n            #It's like adding a pause on my piece, so i have all complete bars and no trucated ones at the end\n            if (self.max_idx + 1) % event_per_bar != 0:\n                self.max_idx += event_per_bar - ((self.max_idx + 1) % event_per_bar) # make sure it has a FULL bar at the end.\n            self.quantised = True\n\n        return\n\n    def simplify_drums(self):\n        ''' use only allowed pitch - and converted not allowed pitch to the similar in a sense of drums!\n        '''\n        #Here forces conversion into the pitches in drum_conversion\n        for note in self.notes:\n            if note.pitch in drum_conversion: # ignore those not included in the key\n                note.pitch = drum_conversion[note.pitch]\n        #https://stackoverflow.com/questions/30670310/what-do-brackets-in-a-for-loop-in-python-mean\n        #The following one is a list comprehension. Basically generates a new list from an existing one using a given condition on the elements\n        self.notes = [note for note in self.notes if note.pitch in allowed_pitch]\t\n\n        return\n\n    def return_as_text(self):\n        ''''''\n        length = int(self.max_idx + 1) # of events in the track.\n        #print(type(length))\n        event_track = []\n        #Thw following cycle create a 9 by N matrix. I append N times a vector of nine zeros.\n        #This means that i create N notes, and then i initialize them with all zeros (9 zeros, since a note is represented by a 9 element binary number)\n\n        for note_idx in range(length):  #sostituire xrange con range in Python3\n            event_track.append(['0']*len(allowed_pitch))\n\n        num_bars = length/event_per_bar# + ceil(len(event_texts_temp) % _event_per_bar)\n\n        for note in self.notes:\n            pitch_here = note.pitch\n            #The following line returns the index of the passed pitch. Basically given an input generic pitch\n            #it returns the associated pitch in my vocabolary (computes the actual mapping from the whole\n            #vocabolary of notes into my reduced one)\n            note_add_pitch_index = allowed_pitch.index(pitch_here) # 0-8\n            #print(type(note.idx))  \n            #print(type(note_add_pitch_index))\n            event_track[int(note.idx)][note_add_pitch_index] = '1'\n            # print note.idx, note.c_tick, note_add_pitch_index, ''.join(event_track[note.idx])\n            # pdb.set_trace()\n\n        event_text_temp = ['0b'+''.join(e) for e in event_track] # encoding to binary\n\n        event_text = []\n        # event_text.append('SONG_BEGIN')\n        # event_text.append('BAR')\n        print(num_bars)\n        print(type(num_bars))        \n        for bar_idx in range(int(num_bars)):\n            event_from = bar_idx * event_per_bar\n            event_to = event_from + event_per_bar\n            event_text = event_text + event_text_temp[event_from:event_to]\n            event_text.append('BAR')\n\n        # event_text.append('SONG_END')\n\n        return ' '.join(event_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function that converts txt to notes. The note is represented as a number (in the MIDI scale)\n\n#in encoded drums ho una riga intera dal file (quindi i vari 0xb00101110) \ndef text_to_notes(encoded_drums, note_list=None):\n    ''' \n    0b0000000000 0b10000000 ...  -> corresponding note. \n    '''\n    if note_list == None:\n        note_list = Note_List()\n#https://www.programiz.com/python-programming/methods/built-in/enumerate enumerate mi ritorna coppie di (indice,valore) \n    for word_idx, word in enumerate(encoded_drums):\n        c_tick_here = word_idx*min_ppq \n\n        for pitch_idx, pitch in enumerate(allowed_pitch):\n\n            if word[pitch_idx+2] == '1':\n                new_note = Note(pitch, c_tick_here)\n                note_list.add_note(new_note)\n    return note_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Il prossimo pezzo di codice serve solo per debug, codice corretto in fondo"},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import randint\n\ndef conv_text_to_midi(filename):\n    # generate some integers\n    idMidi = randint(0, 10)\n    #if os.path.exists(filename[:-4]+'.mid'):\n    if os.path.exists('/kaggle/working/output'+ str(idMidi)+ '.mid'):\n        print(\"Midi già esistente\")\n        return\n    f = open(filename, 'r')\n    #These multiple readlines are actually useless. Need to check the output of the NN, but right now they're useless.\n    #One single readline is enough\n    #f.readline() # title\n    #f.readline() # seed sentence\n    #legge una riga intera dal file\n    sentence = f.readline()\n    #splitta gli elementi letti a ogni spazio.\n    encoded_drums = sentence.split(' ')\n    print('printing encoded drums')\n    print(encoded_drums)\n    #find the first BAR\n\n    first_bar_idx = encoded_drums.index('BAR') \n\n    #encoded_drums = encoded_drums[first_bar_idx:]\n    try:\n        encoded_drums = [ele for ele in encoded_drums if ele not in ['BAR', 'SONG_BEGIN', 'SONG_END', '']]\n    except:\n        pdb.set_trace()\n\n    # prepare output\n    note_list = Note_List()\n    pattern = Containers.Pattern(fmt=0) #Don't know why there's an assertion in the code for fmt=0 if Pattern.len < 1\n    track = Containers.Track()\n    #??\n    PPQ = 480\n    min_ppq = PPQ / (event_per_bar/4)\n    track.resolution = PPQ # ???? too slow. why??\n    pattern.resolution = PPQ\n    # track.resolution = 192\n    pattern.append(track)\n\n    velocity = 84\n    duration = min_ppq*9/10  # it is easier to set new ticks if duration is shorter than _min_ppq\n\n    note_list = text_to_notes(encoded_drums, note_list=note_list)\n\n    max_c_tick = 0 \n    not_yet_offed = [] # set of midi.pitch object\n    print('entering for note_idx cycle')\n    #In this cycle im adding all the notes except the very last one\n    for note_idx, note in enumerate(note_list.notes[:-1]):\n        # add onset\n        tick_here = note.c_tick - max_c_tick #extracting relative tick\n        pitch_here = pitch_to_midipitch[note.pitch]\n        # if pitch_here in cymbals_pitch: # \"Lazy-off\" for cymbals \n        # \toff = midi.NoteOffEvent(tick=0, pitch=pitch_here)\n        # \ttrack.append(off)\n\n        on = Events.NoteOnEvent(tick=tick_here, velocity=velocity, pitch=pitch_here)\n        track.append(on)\n        max_c_tick = max(max_c_tick, note.c_tick)\n        # add offset for something not cymbal\n\n        # if note_list.notes[note_idx+1].c_tick == note.c_tick:\n        # \tif pitch_here not in cymbals_pitch:\n        # \t# \tnot_yet_offed.append(pitch_here)\n\n        # else:\n        # check out some note that not off-ed.\n        \n        #Never enters this cycle. It's useless since we're not adding a NoteOff event anymore. We don't need that\n        for off_idx, waiting_pitch in enumerate(not_yet_offed):\n            print(off_idx)\n            if off_idx == 0:\n                off = Events.NoteOffEvent(tick=duration, pitch=waiting_pitch)\n                max_c_tick = max_c_tick + duration\n            else:\n                print('appending end note')\n                off = Events.NoteOffEvent(tick=0, pitch=waiting_pitch)\n            track.append(off)\n            not_yet_offed = [] # set of midi.pitch object \n\n    # finalise\n    if note_list.notes == []:\n        print ('No notes in %s' % filename)\n        return\n        pdb.set_trace()\n    #here i'm going to add the last note and close the track with the EndEvent\n    note = note_list.notes[-1]\n    tick_here = note.c_tick - max_c_tick\n    pitch_here = pitch_to_midipitch[note.pitch]\n    on = Events.NoteOnEvent(tick=tick_here, velocity=velocity, pitch=pitch_here)\n    off = Events.NoteOffEvent(tick=duration, pitch=pitch_here)\n\n    for off_idx, waiting_pitch in enumerate(not_yet_offed):\n        off = Events.NoteOffEvent(tick=0, pitch=waiting_pitch)\n\n    # end of track event\n    eot = Events.EndOfTrackEvent(tick=1)\n    track.append(eot)\n    # print pattern\n    #print(pattern)\n    \n    #FileIO.write_midifile(filename[:-4]+'.mid', pattern)\n    FileIO.write_midifile('/kaggle/working/output' +str(idMidi)+ '.mid', pattern)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conv_text_to_midi(\"../input/testdata/test deiversity 1.txt\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alternativamente e meglio"},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_text_to_midi(filename):\n    if os.path.exists(filename[:-4]+'.mid'):\n        print(\"Midi già esistente\")\n        return\n    f = open(filename, 'r')\n    #These multiple readlines are actually useless. Need to check the output of the NN, but right now they're useless.\n    #One single readline is enough\n    #f.readline() # title\n    #f.readline() # seed sentence\n    #legge una riga intera dal file\n    sentence = f.readline()\n    #splitta gli elementi letti a ogni spazio.\n    encoded_drums = sentence.split(' ')\n    print('printing encoded drums')\n    print(encoded_drums)\n    #find the first BAR\n\n    first_bar_idx = encoded_drums.index('BAR') \n\n    #encoded_drums = encoded_drums[first_bar_idx:]\n    try:\n        encoded_drums = [ele for ele in encoded_drums if ele not in ['BAR', 'SONG_BEGIN', 'SONG_END', '']]\n    except:\n        pdb.set_trace()\n\n    # prepare output\n    note_list = Note_List()\n    pattern = Containers.Pattern(fmt=0) #Don't know why there's an assertion in the code for fmt=0 if Pattern.len < 1\n    track = Containers.Track()\n    #??\n    PPQ = 480\n    min_ppq = PPQ / (event_per_bar/4)\n    track.resolution = PPQ # ???? too slow. why??\n    pattern.resolution = PPQ\n    # track.resolution = 192\n    pattern.append(track)\n\n    velocity = 84\n    duration = min_ppq*9/10  # it is easier to set new ticks if duration is shorter than _min_ppq\n\n    note_list = text_to_notes(encoded_drums, note_list=note_list)\n\n    max_c_tick = 0 \n    not_yet_offed = [] # set of midi.pitch object\n    print('entering for note_idx cycle')\n    #In this cycle im adding all the notes except the very last one\n    for note_idx, note in enumerate(note_list.notes[:-1]):\n        # add onset\n        tick_here = note.c_tick - max_c_tick #extracting relative tick\n        pitch_here = pitch_to_midipitch[note.pitch]\n        # if pitch_here in cymbals_pitch: # \"Lazy-off\" for cymbals \n        # \toff = midi.NoteOffEvent(tick=0, pitch=pitch_here)\n        # \ttrack.append(off)\n\n        on = Events.NoteOnEvent(tick=tick_here, velocity=velocity, pitch=pitch_here)\n        track.append(on)\n        max_c_tick = max(max_c_tick, note.c_tick)\n        # add offset for something not cymbal\n\n        # if note_list.notes[note_idx+1].c_tick == note.c_tick:\n        # \tif pitch_here not in cymbals_pitch:\n        # \t# \tnot_yet_offed.append(pitch_here)\n\n        # else:\n        # check out some note that not off-ed.\n        \n        #Never enters this cycle. It's useless since we're not adding a NoteOff event anymore. We don't need that\n        for off_idx, waiting_pitch in enumerate(not_yet_offed):\n            print(off_idx)\n            if off_idx == 0:\n                off = Events.NoteOffEvent(tick=duration, pitch=waiting_pitch)\n                max_c_tick = max_c_tick + duration\n            else:\n                print('appending end note')\n                off = Events.NoteOffEvent(tick=0, pitch=waiting_pitch)\n            track.append(off)\n            not_yet_offed = [] # set of midi.pitch object \n\n    # finalise\n    if note_list.notes == []:\n        print ('No notes in %s' % filename)\n        return\n        pdb.set_trace()\n    #here i'm going to add the last note and close the track with the EndEvent\n    note = note_list.notes[-1]\n    tick_here = note.c_tick - max_c_tick\n    pitch_here = pitch_to_midipitch[note.pitch]\n    on = Events.NoteOnEvent(tick=tick_here, velocity=velocity, pitch=pitch_here)\n    off = Events.NoteOffEvent(tick=duration, pitch=pitch_here)\n\n    for off_idx, waiting_pitch in enumerate(not_yet_offed):\n        off = Events.NoteOffEvent(tick=0, pitch=waiting_pitch)\n\n    # end of track event\n    eot = Events.EndOfTrackEvent(tick=1)\n    track.append(eot)\n    # print pattern\n    #print(pattern)\n    \n    FileIO.write_midifile(filename[:-4]+'.mid', pattern)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conv_text_to_midi(\"./r_word__256_512_unitsresult_word_iter_15_diversity_1.50.txt\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}