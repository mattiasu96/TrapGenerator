{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(maxlen, num_chars, num_layers, num_units):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    for layer_idx in range(num_layers):\n",
    "        if layer_idx == 0:\n",
    "            #the size is maxlen x numchars. Maxlen is because of the length of a sentence (128 words) while num_chars=119\n",
    "            #which are the unique words in my vocabulary.\n",
    "            #NB: the input is one hot encoded over the vocabulary size (num_chars)\n",
    "            model.add(LSTM(num_units, return_sequences=True, input_shape=(maxlen, num_chars)))\n",
    "        else:\n",
    "            model.add(LSTM(num_units, return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(num_chars))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(is_character=False, maxlen=None, num_units=None, model_prefix=''):\n",
    "\n",
    "    character_mode = is_character\n",
    "\n",
    "    if character_mode:\n",
    "        if maxlen == None:\n",
    "            maxlen = 1024\n",
    "        if num_units == None:\n",
    "            num_units = 32\n",
    "        step = 2*17 # step to create training data for truncated-BPTT\n",
    "    else: # word mode\n",
    "        if maxlen == None:\n",
    "            maxlen = 128 # maxlength used in RNN input\n",
    "        if num_units == None: \n",
    "            num_units = 512 #number of unit per layer LSTM 512 \n",
    "        step = 8\n",
    "\n",
    "    if character_mode:\n",
    "        num_char_pred = maxlen*3/2\n",
    "    else: \n",
    "        num_char_pred = 17*30 #this should be the number of elements predicted in the output. How \"long\" is my output sequence\n",
    "\n",
    "    num_layers = 2\n",
    "    # \n",
    "    if character_mode:\n",
    "        prefix = 'char'\n",
    "    else:\n",
    "        prefix = 'word'\n",
    "\n",
    "    path = 'metallica_drums_text.txt' # Corpus file\n",
    "    text = open(path).read()\n",
    "    print('corpus length:', len(text))\n",
    "\n",
    "    if character_mode:\n",
    "        chars = set(text)\n",
    "    else:\n",
    "        chord_seq = text.split(' ')\n",
    "        chars = set(chord_seq) #contains the unique words in my dictionary. They are 119\n",
    "        text = chord_seq #contains the full text in an array format. Each entry of my array is a word of type 0xb0110101010 \n",
    "\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    num_chars = len(char_indices) #number of unique words in my training set\n",
    "    print('total chars:', num_chars)\n",
    "\n",
    "    # cut the text in semi-redundant sequences of maxlen characters\n",
    "\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    #Here im creating the inputs and targets for my RNN. Each single input has length maxlen.\n",
    "    #Inputs are semi-redundant, in the sense that i take a length of maxlen=128 and the step is 8. So my first part of the input\n",
    "    #will be the same and the last 8 elements are \"new\". I'm just \"slitting\" of 8 notes ahead\n",
    "    for i in range(0, len(text) - maxlen, step): #iterates over the range with steps of 8.\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    print('nb sequences:', len(sentences))\n",
    "    print('Vectorization...')\n",
    "    \n",
    "    #Here i'm creating the input dataset and target dataset for my network\n",
    "    #X is a tri-dimensional vector: 1 dimension -> Sentences, 2 dimension -> Single sentence, 3 dimension -> one hot encoded vector of the single word\n",
    "    #So basically i have a structure where i have N sentences of maxlen Words where each word is represented as a one hot vector of length num_chars\n",
    "    X = np.zeros((len(sentences), maxlen, num_chars), dtype=np.bool) #Input matrix\n",
    "    y = np.zeros((len(sentences), num_chars), dtype=np.bool) #Target Matrix\n",
    "    #Here i'm actually \"populating\" the matrixes, which were initialized with all zeros\n",
    "    print('Entering initialization cycle')\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1 #NB: char in this case means a whole word like oxb01011101. With char_indices[char] i'm retrieving the index of my word inside my dictionary of (words,index)\n",
    "        print('Finished input initialization')\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    print('Completed Vectorization')\n",
    "    # build the model: 2 stacked LSTM\n",
    "    model = get_model(maxlen, num_chars, num_layers, num_units) \n",
    "    \n",
    "    #Just some string declarations for folders management and names.\n",
    "    #NB: CHANGE THE / with \\ for windows! \n",
    "    result_directory = 'r_%s_%s_%d_%d_units' % (prefix, model_prefix, maxlen, num_units)\n",
    "    filepath_model = os.path.join(result_directory, 'best_model.hdf')\n",
    "\n",
    "    #filepath_model = '%sbest_model.hdf' % result_directory\n",
    "    description_model = '%s, %d layers, %d units, %d maxlen, %d steps' % (prefix, num_layers, num_units, maxlen, step)\n",
    "    \n",
    "    #Usual Model checkpoints and Early Stopping\n",
    "    checker = tf.keras.callbacks.ModelCheckpoint(filepath_model, monitor='loss', verbose=0, save_best_only=True, mode='auto')\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15, verbose=0, mode='auto')\n",
    "    \n",
    "    #create a result directory if it doesn't exist\n",
    "    if not os.path.exists(result_directory):\n",
    "        os.mkdir(result_directory)\n",
    "\n",
    "    # write a description file.\n",
    "    #creates an empty file with the drscription of my model as title\n",
    "    with open(result_directory+description_model, 'w') as f_description:\n",
    "        pass\n",
    "\n",
    "    # train the model, output generated text after each iteration\n",
    "    batch_size = 128 #Size of a training batch. So basically i'll update my loss function every 128 input sentences (usual batch gradient descent)\n",
    "    loss_history = []\n",
    "    pt_x = [1,29,30,40,100,100,200,300,400]\n",
    "    #An epoch is a complete iteration over the whole input training set. So 10 epochs means that i iterates 10 times over my input dataset\n",
    "    nb_epochs = [np.sum(pt_x[:i+1]) for i in range(len(pt_x))] #array containing many epochs length. The model will be fitted many times, one for each nb_epochs.\n",
    "\n",
    "    # not random seed, but the same seed for all.\n",
    "    #A random seed (or seed state, or just seed) is a number (or vector) used to initialize a pseudorandom number generator.\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for iteration, nb_epoch in zip(pt_x,nb_epochs):\n",
    "        if os.path.exists('stop_asap.keunwoo'):\n",
    "            os.remove('stop_asap.keunwoo')\n",
    "            break\n",
    "\n",
    "        print('-' * 50)\n",
    "        print('Iteration', iteration)\n",
    "        \n",
    "        #fitting model over nb_epochs\n",
    "        result = model.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, callbacks=[checker, early_stop]) \n",
    "        loss_history = loss_history + result.history['loss']\n",
    "\n",
    "        print ('Saving model after %d epochs...' % nb_epoch)\n",
    "        #Saving model weights. Saving a model trained over nb_epochs\n",
    "        model.save_weights('%smodel_after_%d.hdf'%(result_directory, nb_epoch), overwrite=True) \n",
    "\n",
    "        for diversity in [0.9, 1.0, 1.2]:\n",
    "            #creates a .txt file where i will save my predictions\n",
    "            with open(('%sresult_%s_iter_%02d_diversity_%4.2f.txt' % (result_directory, prefix, iteration, diversity)), 'w') as f_write:\n",
    "\n",
    "                print()\n",
    "                print('----- diversity:', diversity)\n",
    "                f_write.write('diversity:%4.2f\\n' % diversity)\n",
    "                if character_mode:\n",
    "                    generated = ''\n",
    "                else:\n",
    "                    generated = [] #simple initialization\n",
    "                #selects a random sentence from my input dataset.\n",
    "                sentence = text[start_index: start_index + maxlen]\n",
    "                seed_sentence = text[start_index: start_index + maxlen]\n",
    "\n",
    "                if character_mode:\n",
    "                    generated += sentence\n",
    "                else:\n",
    "                    #at first iteration i just add my input sentence in my generated element\n",
    "                    generated = generated + sentence\n",
    "\n",
    "\n",
    "                print('----- Generating with seed:')\n",
    "\n",
    "                if character_mode:\n",
    "                    print(sentence)\n",
    "                    sys.stdout.write(generated)\n",
    "                else:\n",
    "                    print(' '.join(sentence))\n",
    "\n",
    "                for i in range(num_char_pred): \n",
    "                    # if generated.endswith('_END_'):\n",
    "                    # \tbreak\n",
    "                    x = np.zeros((1, maxlen, num_chars)) #initialization of input. Matrix of maxlen words, each \n",
    "\n",
    "                    for t, char in enumerate(sentence):\n",
    "                        x[0, t, char_indices[char]] = 1. \n",
    "\n",
    "                    preds = model.predict(x, verbose=0)[0] \n",
    "                    next_index = sample(preds, diversity)\n",
    "                    next_char = indices_char[next_index]\n",
    "\n",
    "                    if character_mode:\n",
    "                        generated += next_char\n",
    "                        sentence = sentence[1:] + next_char\n",
    "                    else:\n",
    "                        generated.append(next_char)\n",
    "                        sentence = sentence[1:]\n",
    "                        sentence.append(next_char)\n",
    "\n",
    "                    if character_mode:\n",
    "                        sys.stdout.write(next_char)\n",
    "                    # else:\n",
    "                    # \tfor ch in next_char:\n",
    "                    # \t\tsys.stdout.write(ch)\t\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                if character_mode:\n",
    "                    f_write.write(seed_sentence + '\\n')\n",
    "                    f_write.write(generated)\n",
    "                else:\n",
    "                    f_write.write(' '.join(seed_sentence) + '\\n')\n",
    "                    f_write.write(' ' .join(generated))\n",
    "\n",
    "        np.save('%sloss_%s.npy'%(result_directory, prefix), loss_history)\n",
    "\n",
    "    print ('Done! You might want to run main_post_process.py to get midi files. ')\n",
    "    print ('You need python-midi (https://github.com/vishnubob/python-midi) to run it.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
