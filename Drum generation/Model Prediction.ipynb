{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(maxlen, num_chars, num_layers, num_units):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    for layer_idx in range(num_layers):\n",
    "        if layer_idx == 0:\n",
    "            #the size is maxlen x numchars. Maxlen is because of the length of a sentence (128 words) while num_chars=119\n",
    "            #which are the unique words in my vocabulary.\n",
    "            #NB: the input is one hot encoded over the vocabulary size (num_chars)\n",
    "            model.add(LSTM(num_units, return_sequences=True, input_shape=(maxlen, num_chars)))\n",
    "        else:\n",
    "            model.add(LSTM(num_units, return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(num_chars))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(maxlen, num_chars, num_layers, num_units):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, num_chars)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(512, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(512, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_chars))\n",
    "    model.add(Activation('softmax'))\n",
    " \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(is_character=False, maxlen=None, num_units=None, model_prefix=''):\n",
    "\n",
    "    character_mode = is_character\n",
    "\n",
    "    if character_mode:\n",
    "        if maxlen == None:\n",
    "            maxlen = 1024\n",
    "        if num_units == None:\n",
    "            num_units = 32\n",
    "        step = 2*17 # step to create training data for truncated-BPTT\n",
    "    else: # word mode\n",
    "        if maxlen == None:\n",
    "            maxlen = 256 # maxlength used in RNN input\n",
    "        if num_units == None: \n",
    "            num_units = 512 #number of unit per layer LSTM 512 \n",
    "        step = 8\n",
    "\n",
    "    if character_mode:\n",
    "        num_char_pred = maxlen*3/2\n",
    "    else: \n",
    "        num_char_pred = 17*30 #this should be the number of elements predicted in the output. How \"long\" is my output sequence\n",
    "\n",
    "    num_layers = 2\n",
    "    # \n",
    "    if character_mode:\n",
    "        prefix = 'char'\n",
    "    else:\n",
    "        prefix = 'word'\n",
    "\n",
    "    path = 'datasetartificial 32note.txt' # Corpus file\n",
    "    text = open(path).read()\n",
    "    print('corpus length:', len(text))\n",
    "\n",
    "    if character_mode:\n",
    "        chars = set(text)\n",
    "    else:\n",
    "        chord_seq = text.split(' ')\n",
    "        chars = set(chord_seq) #contains the unique words in my dictionary. They are 119\n",
    "        text = chord_seq #contains the full text in an array format. Each entry of my array is a word of type 0xb0110101010 \n",
    "\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    num_chars = len(char_indices) #number of unique words in my training set\n",
    "    print('total chars:', num_chars)\n",
    "\n",
    "    # cut the text in semi-redundant sequences of maxlen characters\n",
    "\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    #Here im creating the inputs and targets for my RNN. Each single input has length maxlen.\n",
    "    #Inputs are semi-redundant, in the sense that i take a length of maxlen=128 and the step is 8. So my first part of the input\n",
    "    #will be the same and the last 8 elements are \"new\". I'm just \"slitting\" of 8 notes ahead\n",
    "    for i in range(0, len(text) - maxlen, step): #iterates over the range with steps of 8.\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    print('nb sequences:', len(sentences))\n",
    "    print('Vectorization...')\n",
    "    \n",
    "    #Here i'm creating the input dataset and target dataset for my network\n",
    "    #X is a tri-dimensional vector: 1 dimension -> Sentences, 2 dimension -> Single sentence, 3 dimension -> one hot encoded vector of the single word\n",
    "    #So basically i have a structure where i have N sentences of maxlen Words where each word is represented as a one hot vector of length num_chars\n",
    "    X = np.zeros((len(sentences), maxlen, num_chars), dtype=np.bool) #Input matrix\n",
    "    y = np.zeros((len(sentences), num_chars), dtype=np.bool) #Target Matrix\n",
    "    #Here i'm actually \"populating\" the matrixes, which were initialized with all zeros\n",
    "    print('Entering initialization cycle')\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1 #NB: char in this case means a whole word like oxb01011101. With char_indices[char] i'm retrieving the index of my word inside my dictionary of (words,index)\n",
    "        #print('Finished input initialization')\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    print('Completed Vectorization')\n",
    "    # build the model: 2 stacked LSTM\n",
    "    model = get_model(maxlen, num_chars, num_layers, num_units) \n",
    "    \n",
    "    #Just some string declarations for folders management and names.\n",
    "    #NB: CHANGE THE / with \\ for windows! \n",
    "    result_directory = 'r_%s_%s_%d_%d_units' % (prefix, model_prefix, maxlen, num_units)\n",
    "    filepath_model = os.path.join(result_directory, 'best_model.hdf')\n",
    "\n",
    "    #filepath_model = '%sbest_model.hdf' % result_directory\n",
    "    description_model = '%s, %d layers, %d units, %d maxlen, %d steps' % (prefix, num_layers, num_units, maxlen, step)\n",
    "    \n",
    "    #Usual Model checkpoints and Early Stopping\n",
    "    checker = tf.keras.callbacks.ModelCheckpoint(filepath_model, monitor='loss', verbose=0, save_best_only=True, mode='auto')\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15, verbose=0, mode='auto')\n",
    "    \n",
    "    #create a result directory if it doesn't exist\n",
    "    if not os.path.exists(result_directory):\n",
    "        os.mkdir(result_directory)\n",
    "\n",
    "    # write a description file.\n",
    "    #creates an empty file with the drscription of my model as title\n",
    "    with open(result_directory+description_model, 'w') as f_description:\n",
    "        pass\n",
    "\n",
    "    # train the model, output generated text after each iteration\n",
    "    batch_size = 128 #Size of a training batch. So basically i'll update my loss function every 128 input sentences (usual batch gradient descent)\n",
    "    loss_history = []\n",
    "    pt_x = [1]\n",
    "    #An epoch is a complete iteration over the whole input training set. So 10 epochs means that i iterates 10 times over my input dataset\n",
    "    nb_epochs = [np.sum(pt_x[:i+1]) for i in range(len(pt_x))] #array containing many epochs length. The model will be fitted many times, one for each nb_epochs.\n",
    "\n",
    "    # not random seed, but the same seed for all.\n",
    "    #A random seed (or seed state, or just seed) is a number (or vector) used to initialize a pseudorandom number generator.\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for iteration, nb_epoch in zip(pt_x,nb_epochs):\n",
    "        if os.path.exists('stop_asap.keunwoo'):\n",
    "            os.remove('stop_asap.keunwoo')\n",
    "            break\n",
    "\n",
    "        print('-' * 50)\n",
    "        print('Iteration', iteration)\n",
    "        \n",
    "        #fitting model over nb_epochs\n",
    "        #result = model.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, callbacks=[checker, early_stop]) \n",
    "        #loss_history = loss_history + result.history['loss']\n",
    "\n",
    "        print ('Saving model after %d epochs...' % nb_epoch)\n",
    "        #Saving model weights. Saving a model trained over nb_epochs\n",
    "        #model.save_weights('%smodel_after_%d.hdf'%(result_directory, nb_epoch), overwrite=True) \n",
    "        model.load_weights(\"model_weights_32.h5\")\n",
    "        w2 = model.get_weights()\n",
    "        print(w2)\n",
    "\n",
    "        \n",
    "        for diversity in [0.9, 1.0, 1.2]:\n",
    "            #creates a .txt file where i will save my predictions\n",
    "            with open(('%sresult_%s_iter_%02d_diversity_%4.2f.txt' % (result_directory, prefix, iteration, diversity)), 'w') as f_write:\n",
    "\n",
    "                print()\n",
    "                print('----- diversity:', diversity)\n",
    "                f_write.write('diversity:%4.2f\\n' % diversity)\n",
    "                if character_mode:\n",
    "                    generated = ''\n",
    "                else:\n",
    "                    generated = [] #simple initialization\n",
    "                #selects a random sentence from my input dataset.\n",
    "                sentence = text[start_index: start_index + maxlen]\n",
    "                seed_sentence = text[start_index: start_index + maxlen]\n",
    "\n",
    "                if character_mode:\n",
    "                    generated += sentence\n",
    "                else:\n",
    "                    #at first iteration i just add my input sentence in my generated element\n",
    "                    generated = generated + sentence\n",
    "\n",
    "\n",
    "                print('----- Generating with seed:')\n",
    "\n",
    "                if character_mode:\n",
    "                    print(sentence)\n",
    "                    sys.stdout.write(generated)\n",
    "                else:\n",
    "                    print(' '.join(sentence))\n",
    "\n",
    "                for i in range(num_char_pred): \n",
    "                    # if generated.endswith('_END_'):\n",
    "                    # \tbreak\n",
    "                    x = np.zeros((1, maxlen, num_chars)) #initialization of input. Matrix of maxlen words, each \n",
    "\n",
    "                    for t, char in enumerate(sentence):\n",
    "                        x[0, t, char_indices[char]] = 1. \n",
    "\n",
    "                    preds = model.predict(x, verbose=0)[0] \n",
    "                    #print('printo la prediction')\n",
    "                    #print(preds)\n",
    "                    next_index = sample(preds, diversity)\n",
    "                    next_char = indices_char[next_index]\n",
    "                    #print('printo il next char')\n",
    "                    #print(next_char)\n",
    "\n",
    "                    if character_mode:\n",
    "                        generated += next_char\n",
    "                        sentence = sentence[1:] + next_char\n",
    "                    else:\n",
    "                        generated.append(next_char)\n",
    "                        sentence = sentence[1:]\n",
    "                        sentence.append(next_char)\n",
    "                        #print('printo la sentence generata')\n",
    "                        #print(generated)\n",
    "\n",
    "                    if character_mode:\n",
    "                        sys.stdout.write(next_char)\n",
    "                    # else:\n",
    "                    # \tfor ch in next_char:\n",
    "                    # \t\tsys.stdout.write(ch)\t\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                if character_mode:\n",
    "                    f_write.write(seed_sentence + '\\n')\n",
    "                    f_write.write(generated)\n",
    "                else:\n",
    "                    f_write.write(' '.join(seed_sentence))\n",
    "                    \n",
    "                    f_write.write(' ' .join(generated))\n",
    "\n",
    "        np.save('%sloss_%s.npy'%(result_directory, prefix), loss_history)\n",
    "\n",
    "    print ('Done! You might want to run main_post_process.py to get midi files. ')\n",
    "    print ('You need python-midi (https://github.com/vishnubob/python-midi) to run it.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 7759999\n",
      "total chars: 9\n",
      "nb sequences: 82468\n",
      "Vectorization...\n",
      "Entering initialization cycle\n",
      "Completed Vectorization\n",
      "Build model...\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Saving model after 1 epochs...\n",
      "[array([[ 0.14722891,  0.6613739 , -0.6011135 , ..., -0.22331041,\n",
      "         0.22202273,  0.5753048 ],\n",
      "       [-0.5218361 , -0.5189415 , -0.14500439, ..., -0.48617923,\n",
      "        -0.36494222,  0.5684956 ],\n",
      "       [-0.19060896, -0.43396616, -0.4240928 , ..., -0.32273486,\n",
      "         0.27929464,  0.13415767],\n",
      "       ...,\n",
      "       [ 0.04526247, -0.0097519 , -0.22986586, ..., -0.29404777,\n",
      "        -0.3430315 , -0.45311692],\n",
      "       [ 0.19368096, -0.09979554, -0.24670413, ..., -0.31789327,\n",
      "         0.47094792, -0.36561748],\n",
      "       [-0.01649765,  0.10508529,  0.08910104, ..., -0.08903371,\n",
      "        -0.30781072, -0.26704594]], dtype=float32), array([[ 0.0024009 , -0.05633045, -0.1241108 , ..., -0.15300463,\n",
      "        -0.01655586, -0.1606396 ],\n",
      "       [-0.0756053 , -0.08771748,  0.03854953, ...,  0.32876068,\n",
      "        -0.01514403,  0.1655511 ],\n",
      "       [ 0.08401804,  0.2979694 ,  0.12984103, ...,  0.10328498,\n",
      "         0.1133269 , -0.01898988],\n",
      "       ...,\n",
      "       [ 0.02903414,  0.00178253, -0.07047229, ...,  0.15778567,\n",
      "         0.0980821 ,  0.09326889],\n",
      "       [-0.13389707,  0.02534981,  0.06265391, ..., -0.02607308,\n",
      "         0.01540086,  0.24936603],\n",
      "       [-0.00688998, -0.15624115, -0.13152446, ...,  0.0280468 ,\n",
      "        -0.00434336, -0.00810625]], dtype=float32), array([-0.14493099, -0.06471507, -0.3924672 , ..., -0.45955753,\n",
      "       -0.20230015, -0.160102  ], dtype=float32), array([[ 0.07422296,  0.03745496,  0.06094288, ..., -0.07632335,\n",
      "         0.20754355,  0.09586105],\n",
      "       [ 0.17971261, -0.00932257, -0.39107963, ..., -0.15014625,\n",
      "        -0.01248396,  0.06072558],\n",
      "       [ 0.1772013 , -0.18718255,  0.31839088, ...,  0.03489288,\n",
      "        -0.1515052 , -0.102501  ],\n",
      "       ...,\n",
      "       [ 0.25212038, -0.0359632 ,  0.24027044, ..., -0.24814811,\n",
      "        -0.08361783, -0.41587204],\n",
      "       [ 0.1251052 ,  0.04840981,  0.09741107, ..., -0.02025478,\n",
      "        -0.04922619, -0.07133454],\n",
      "       [-0.3134834 , -0.05678357,  0.1591529 , ...,  0.13885707,\n",
      "         0.07350545,  0.1568241 ]], dtype=float32), array([[-0.12123272, -0.03858231,  0.03587656, ..., -0.0966115 ,\n",
      "         0.20024525, -0.03217158],\n",
      "       [ 0.289151  , -0.33115858,  0.02731367, ..., -0.04394293,\n",
      "         0.07845405, -0.17276539],\n",
      "       [-0.10491755,  0.3794017 , -0.04983938, ...,  0.19092159,\n",
      "         0.04445567,  0.06411903],\n",
      "       ...,\n",
      "       [ 0.43588996,  0.02312208, -0.2619351 , ...,  0.06264991,\n",
      "         0.11097999,  0.09096772],\n",
      "       [-0.05595774,  0.20411277,  0.04237304, ...,  0.11953407,\n",
      "        -0.01578027, -0.28720596],\n",
      "       [ 0.00755618, -0.3188518 , -0.13509744, ...,  0.35969892,\n",
      "         0.17873788,  0.06233141]], dtype=float32), array([-0.03383483,  0.0779098 ,  0.00507966, ...,  0.01798548,\n",
      "       -0.27171534, -0.23565412], dtype=float32), array([[-0.27607134,  0.03300906,  0.21539187, ..., -0.32830203,\n",
      "        -0.0462532 , -0.34421712],\n",
      "       [-0.02434857,  0.25936714, -0.3399839 , ..., -0.39475015,\n",
      "        -0.19553277, -0.3096076 ],\n",
      "       [-0.0090509 ,  0.19870418,  0.14320762, ..., -0.22725126,\n",
      "         0.24887161, -0.12151621],\n",
      "       ...,\n",
      "       [ 0.24107586,  0.09719663, -0.18962497, ..., -0.1158044 ,\n",
      "        -0.00301055, -0.04838326],\n",
      "       [-0.17319815,  0.00362179,  0.24475388, ...,  0.11358071,\n",
      "        -0.31130284, -0.14806034],\n",
      "       [ 0.02028525,  0.125462  , -0.1626165 , ..., -0.15391172,\n",
      "         0.4544352 ,  0.16286881]], dtype=float32), array([-0.1497988 ,  0.01047161, -0.16818948,  0.20532033, -0.16651474,\n",
      "       -0.0344329 ,  0.13528644, -0.37615922, -0.11181835], dtype=float32)]\n",
      "\n",
      "----- diversity: 0.9\n",
      "----- Generating with seed:\n",
      "0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b000000000 0b111000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b010000000 0b001000000 0b001000000 0b010000000 0b101000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b000000000 0b111000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b010000000 0b001000000 0b001000000 0b010000000\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed:\n",
      "0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b000000000 0b111000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b010000000 0b001000000 0b001000000 0b010000000 0b101000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b000000000 0b111000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b010000000 0b001000000 0b001000000 0b010000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed:\n",
      "0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b000000000 0b111000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b010000000 0b001000000 0b001000000 0b010000000 0b101000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b000000000 0b111000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b001000000 0b001000000 0b001000000 0b001000000 BAR 0b101000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b101000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b010000000 0b000000000 0b000000000 0b010000000 0b001000000 0b001000000 0b010000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ec9775ede022>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-b768325ec0a4>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(is_character, maxlen, num_units, model_prefix)\u001b[0m\n\u001b[0;32m    164\u001b[0m                         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m                     \u001b[1;31m#print('printo la prediction')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                     \u001b[1;31m#print(preds)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m     dataset = dataset.map(\n\u001b[1;32m--> 390\u001b[1;33m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[1;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m   1589\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m       return ParallelMapDataset(\n\u001b[1;32m-> 1591\u001b[1;33m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   3924\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3925\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3926\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   3927\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[0;32m   3928\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    896\u001b[0m       \u001b[0mkwarg_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     func_args = _get_defun_inputs_from_args(\n\u001b[1;32m--> 898\u001b[1;33m         args, arg_names, flat_shapes=arg_shapes)\n\u001b[0m\u001b[0;32m    899\u001b[0m     func_kwargs = _get_defun_inputs_from_kwargs(\n\u001b[0;32m    900\u001b[0m         kwargs, flat_shapes=kwarg_shapes)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs_from_args\u001b[1;34m(args, names, flat_shapes)\u001b[0m\n\u001b[0;32m   1125\u001b[0m   \u001b[1;34m\"\"\"Maps Python function positional args to graph-construction inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m   return _get_defun_inputs(\n\u001b[1;32m-> 1127\u001b[1;33m       args, names, structure=args, flat_shapes=flat_shapes)\n\u001b[0m\u001b[0;32m   1128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs\u001b[1;34m(args, names, structure, flat_shapes)\u001b[0m\n\u001b[0;32m   1215\u001b[0m           placeholder.op._set_attr(  # pylint: disable=protected-access\n\u001b[0;32m   1216\u001b[0m               \u001b[1;34m\"_user_specified_name\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m               attr_value_pb2.AttrValue(s=compat.as_bytes(requested_name)))\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[0mfunction_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m       elif isinstance(arg, (resource_variable_ops.BaseResourceVariable,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_set_attr\u001b[1;34m(self, attr_name, attr_value)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         compat.as_bytes(attr_value.SerializeToString()))\n\u001b[0;32m   2267\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_attr_with_buf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2269\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m       \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\TrapGenerator\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_set_attr_with_buf\u001b[1;34m(self, attr_name, attr_buf)\u001b[0m\n\u001b[0;32m   2273\u001b[0m     \u001b[1;34m\"\"\"Set an attr in the node_def with a pre-allocated buffer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2274\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2275\u001b[1;33m     \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetAttr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_buf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2276\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
