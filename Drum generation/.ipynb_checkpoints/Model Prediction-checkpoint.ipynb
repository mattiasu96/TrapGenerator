{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(maxlen, num_chars, num_layers, num_units):\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    for layer_idx in range(num_layers):\n",
    "        if layer_idx == 0:\n",
    "            #the size is maxlen x numchars. Maxlen is because of the length of a sentence (128 words) while num_chars=119\n",
    "            #which are the unique words in my vocabulary.\n",
    "            #NB: the input is one hot encoded over the vocabulary size (num_chars)\n",
    "            model.add(LSTM(num_units, return_sequences=True, input_shape=(maxlen, num_chars)))\n",
    "        else:\n",
    "            model.add(LSTM(num_units, return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(num_chars))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(is_character=False, maxlen=None, num_units=None, model_prefix=''):\n",
    "\n",
    "    character_mode = is_character\n",
    "\n",
    "    if character_mode:\n",
    "        if maxlen == None:\n",
    "            maxlen = 1024\n",
    "        if num_units == None:\n",
    "            num_units = 32\n",
    "        step = 2*17 # step to create training data for truncated-BPTT\n",
    "    else: # word mode\n",
    "        if maxlen == None:\n",
    "            maxlen = 128 # maxlength used in RNN input\n",
    "        if num_units == None: \n",
    "            num_units = 512 #number of unit per layer LSTM 512 \n",
    "        step = 8\n",
    "\n",
    "    if character_mode:\n",
    "        num_char_pred = maxlen*3/2\n",
    "    else: \n",
    "        num_char_pred = 17*30 #this should be the number of elements predicted in the output. How \"long\" is my output sequence\n",
    "\n",
    "    num_layers = 2\n",
    "    # \n",
    "    if character_mode:\n",
    "        prefix = 'char'\n",
    "    else:\n",
    "        prefix = 'word'\n",
    "\n",
    "    path = 'sample.txt' # Corpus file\n",
    "    text = open(path).read()\n",
    "    print('corpus length:', len(text))\n",
    "\n",
    "    if character_mode:\n",
    "        chars = set(text)\n",
    "    else:\n",
    "        chord_seq = text.split(' ')\n",
    "        chars = set(chord_seq) #contains the unique words in my dictionary. They are 119\n",
    "        text = chord_seq #contains the full text in an array format. Each entry of my array is a word of type 0xb0110101010 \n",
    "\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    num_chars = len(char_indices) #number of unique words in my training set\n",
    "    print('total chars:', num_chars)\n",
    "\n",
    "    # cut the text in semi-redundant sequences of maxlen characters\n",
    "\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    #Here im creating the inputs and targets for my RNN. Each single input has length maxlen.\n",
    "    #Inputs are semi-redundant, in the sense that i take a length of maxlen=128 and the step is 8. So my first part of the input\n",
    "    #will be the same and the last 8 elements are \"new\". I'm just \"slitting\" of 8 notes ahead\n",
    "    for i in range(0, len(text) - maxlen, step): #iterates over the range with steps of 8.\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    print('nb sequences:', len(sentences))\n",
    "    print('Vectorization...')\n",
    "    \n",
    "    #Here i'm creating the input dataset and target dataset for my network\n",
    "    #X is a tri-dimensional vector: 1 dimension -> Sentences, 2 dimension -> Single sentence, 3 dimension -> one hot encoded vector of the single word\n",
    "    #So basically i have a structure where i have N sentences of maxlen Words where each word is represented as a one hot vector of length num_chars\n",
    "    X = np.zeros((len(sentences), maxlen, num_chars), dtype=np.bool) #Input matrix\n",
    "    y = np.zeros((len(sentences), num_chars), dtype=np.bool) #Target Matrix\n",
    "    #Here i'm actually \"populating\" the matrixes, which were initialized with all zeros\n",
    "    print('Entering initialization cycle')\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1 #NB: char in this case means a whole word like oxb01011101. With char_indices[char] i'm retrieving the index of my word inside my dictionary of (words,index)\n",
    "        #print('Finished input initialization')\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    print('Completed Vectorization')\n",
    "    # build the model: 2 stacked LSTM\n",
    "    model = get_model(maxlen, num_chars, num_layers, num_units) \n",
    "    \n",
    "    #Just some string declarations for folders management and names.\n",
    "    #NB: CHANGE THE / with \\ for windows! \n",
    "    result_directory = 'r_%s_%s_%d_%d_units' % (prefix, model_prefix, maxlen, num_units)\n",
    "    filepath_model = os.path.join(result_directory, 'best_model.hdf')\n",
    "\n",
    "    #filepath_model = '%sbest_model.hdf' % result_directory\n",
    "    description_model = '%s, %d layers, %d units, %d maxlen, %d steps' % (prefix, num_layers, num_units, maxlen, step)\n",
    "    \n",
    "    #Usual Model checkpoints and Early Stopping\n",
    "    checker = tf.keras.callbacks.ModelCheckpoint(filepath_model, monitor='loss', verbose=0, save_best_only=True, mode='auto')\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15, verbose=0, mode='auto')\n",
    "    \n",
    "    #create a result directory if it doesn't exist\n",
    "    if not os.path.exists(result_directory):\n",
    "        os.mkdir(result_directory)\n",
    "\n",
    "    # write a description file.\n",
    "    #creates an empty file with the drscription of my model as title\n",
    "    with open(result_directory+description_model, 'w') as f_description:\n",
    "        pass\n",
    "\n",
    "    # train the model, output generated text after each iteration\n",
    "    batch_size = 128 #Size of a training batch. So basically i'll update my loss function every 128 input sentences (usual batch gradient descent)\n",
    "    loss_history = []\n",
    "    pt_x = [1]\n",
    "    #An epoch is a complete iteration over the whole input training set. So 10 epochs means that i iterates 10 times over my input dataset\n",
    "    nb_epochs = [np.sum(pt_x[:i+1]) for i in range(len(pt_x))] #array containing many epochs length. The model will be fitted many times, one for each nb_epochs.\n",
    "\n",
    "    # not random seed, but the same seed for all.\n",
    "    #A random seed (or seed state, or just seed) is a number (or vector) used to initialize a pseudorandom number generator.\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for iteration, nb_epoch in zip(pt_x,nb_epochs):\n",
    "        if os.path.exists('stop_asap.keunwoo'):\n",
    "            os.remove('stop_asap.keunwoo')\n",
    "            break\n",
    "\n",
    "        print('-' * 50)\n",
    "        print('Iteration', iteration)\n",
    "        \n",
    "        #fitting model over nb_epochs\n",
    "        #result = model.fit(X, y, batch_size=batch_size, nb_epoch=nb_epoch, callbacks=[checker, early_stop]) \n",
    "        #loss_history = loss_history + result.history['loss']\n",
    "\n",
    "        print ('Saving model after %d epochs...' % nb_epoch)\n",
    "        #Saving model weights. Saving a model trained over nb_epochs\n",
    "        #model.save_weights('%smodel_after_%d.hdf'%(result_directory, nb_epoch), overwrite=True) \n",
    "        model.load_weights(\"model_weights.h5\")\n",
    "        w2 = model.get_weights()\n",
    "        print(w2)\n",
    "\n",
    "        \n",
    "        for diversity in [0.9, 1.0, 1.2]:\n",
    "            #creates a .txt file where i will save my predictions\n",
    "            with open(('%sresult_%s_iter_%02d_diversity_%4.2f.txt' % (result_directory, prefix, iteration, diversity)), 'w') as f_write:\n",
    "\n",
    "                print()\n",
    "                print('----- diversity:', diversity)\n",
    "                f_write.write('diversity:%4.2f\\n' % diversity)\n",
    "                if character_mode:\n",
    "                    generated = ''\n",
    "                else:\n",
    "                    generated = [] #simple initialization\n",
    "                #selects a random sentence from my input dataset.\n",
    "                sentence = text[start_index: start_index + maxlen]\n",
    "                seed_sentence = text[start_index: start_index + maxlen]\n",
    "\n",
    "                if character_mode:\n",
    "                    generated += sentence\n",
    "                else:\n",
    "                    #at first iteration i just add my input sentence in my generated element\n",
    "                    generated = generated + sentence\n",
    "\n",
    "\n",
    "                print('----- Generating with seed:')\n",
    "\n",
    "                if character_mode:\n",
    "                    print(sentence)\n",
    "                    sys.stdout.write(generated)\n",
    "                else:\n",
    "                    print(' '.join(sentence))\n",
    "\n",
    "                for i in range(num_char_pred): \n",
    "                    # if generated.endswith('_END_'):\n",
    "                    # \tbreak\n",
    "                    x = np.zeros((1, maxlen, num_chars)) #initialization of input. Matrix of maxlen words, each \n",
    "\n",
    "                    for t, char in enumerate(sentence):\n",
    "                        x[0, t, char_indices[char]] = 1. \n",
    "\n",
    "                    preds = model.predict(x, verbose=0)[0] \n",
    "                    #print('printo la prediction')\n",
    "                    #print(preds)\n",
    "                    next_index = sample(preds, diversity)\n",
    "                    next_char = indices_char[next_index]\n",
    "                    #print('printo il next char')\n",
    "                    #print(next_char)\n",
    "\n",
    "                    if character_mode:\n",
    "                        generated += next_char\n",
    "                        sentence = sentence[1:] + next_char\n",
    "                    else:\n",
    "                        generated.append(next_char)\n",
    "                        sentence = sentence[1:]\n",
    "                        sentence.append(next_char)\n",
    "                        #print('printo la sentence generata')\n",
    "                        #print(generated)\n",
    "\n",
    "                    if character_mode:\n",
    "                        sys.stdout.write(next_char)\n",
    "                    # else:\n",
    "                    # \tfor ch in next_char:\n",
    "                    # \t\tsys.stdout.write(ch)\t\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                if character_mode:\n",
    "                    f_write.write(seed_sentence + '\\n')\n",
    "                    f_write.write(generated)\n",
    "                else:\n",
    "                    f_write.write(' '.join(seed_sentence))\n",
    "                    \n",
    "                    f_write.write(' ' .join(generated))\n",
    "\n",
    "        np.save('%sloss_%s.npy'%(result_directory, prefix), loss_history)\n",
    "\n",
    "    print ('Done! You might want to run main_post_process.py to get midi files. ')\n",
    "    print ('You need python-midi (https://github.com/vishnubob/python-midi) to run it.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 1645119\n",
      "total chars: 8\n",
      "nb sequences: 17474\n",
      "Vectorization...\n",
      "Entering initialization cycle\n",
      "Completed Vectorization\n",
      "Build model...\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Saving model after 1 epochs...\n",
      "[array([[ 0.09525122,  0.1248017 ,  0.00790774, ..., -0.33113068,\n",
      "         0.43234923,  0.07664489],\n",
      "       [ 0.11488641,  0.37064505, -0.01315008, ...,  0.02655961,\n",
      "         0.00837912,  0.22002794],\n",
      "       [ 0.09676978,  0.00113445,  0.03227072, ..., -0.11081001,\n",
      "         0.00392458, -0.16576645],\n",
      "       ...,\n",
      "       [ 0.08325733, -0.0244416 ,  0.08085658, ..., -0.21718177,\n",
      "        -0.30434266, -0.03691684],\n",
      "       [-0.1039081 ,  0.01248645, -0.09937579, ..., -0.17753199,\n",
      "        -0.04743329, -0.08579629],\n",
      "       [ 0.13458267, -0.18847196,  0.03884757, ..., -0.03548539,\n",
      "        -0.10369555, -0.16462715]], dtype=float32), array([[ 0.01358436,  0.00376944,  0.04017671, ..., -0.01777982,\n",
      "         0.05609559,  0.07101183],\n",
      "       [ 0.09799481,  0.03280447, -0.072827  , ..., -0.05052435,\n",
      "         0.00628901, -0.05354229],\n",
      "       [-0.04800365, -0.1010978 , -0.0317899 , ..., -0.0129415 ,\n",
      "        -0.01091371, -0.07933431],\n",
      "       ...,\n",
      "       [ 0.03319461, -0.03084639, -0.03941393, ...,  0.06029299,\n",
      "         0.13656814, -0.01016072],\n",
      "       [-0.00488585, -0.09856561,  0.08422794, ...,  0.09848213,\n",
      "        -0.09508738,  0.05342519],\n",
      "       [ 0.00207828,  0.01487195, -0.06284516, ..., -0.07116032,\n",
      "         0.02501083, -0.03853307]], dtype=float32), array([-0.06378436,  0.02049603, -0.02663055, ..., -0.14613767,\n",
      "       -0.01944925, -0.10617597], dtype=float32), array([[ 0.00719725, -0.0544853 ,  0.04987978, ...,  0.0488194 ,\n",
      "         0.04935532,  0.09771132],\n",
      "       [ 0.12940311,  0.01702308,  0.14962529, ..., -0.00591878,\n",
      "        -0.00435678,  0.01404595],\n",
      "       [-0.05962978, -0.0303227 , -0.14880708, ..., -0.07277958,\n",
      "        -0.01214233, -0.1423527 ],\n",
      "       ...,\n",
      "       [-0.07076061, -0.05675481,  0.0503503 , ...,  0.11069745,\n",
      "         0.00521184, -0.02731998],\n",
      "       [ 0.05850001,  0.12063444, -0.03892226, ...,  0.00066777,\n",
      "         0.00859786,  0.03964136],\n",
      "       [-0.00088667, -0.06991982, -0.04183154, ..., -0.16557586,\n",
      "         0.01242274, -0.09817862]], dtype=float32), array([[ 0.03185437, -0.01993736,  0.03231502, ...,  0.09273114,\n",
      "        -0.01446186,  0.02463607],\n",
      "       [ 0.09530348, -0.01059108, -0.16584371, ..., -0.04374103,\n",
      "        -0.03324587, -0.05729145],\n",
      "       [-0.00594138, -0.01456031, -0.05077501, ...,  0.06000969,\n",
      "         0.0521494 ,  0.03011121],\n",
      "       ...,\n",
      "       [-0.09447941, -0.03275335,  0.01335641, ..., -0.0783667 ,\n",
      "        -0.01257802,  0.02956993],\n",
      "       [-0.08854073,  0.07557082,  0.02771008, ...,  0.10128009,\n",
      "         0.06246744,  0.04386773],\n",
      "       [-0.01839618, -0.0408595 , -0.00608558, ...,  0.04177684,\n",
      "        -0.01906491, -0.01618586]], dtype=float32), array([ 0.01454907, -0.04117675, -0.01240289, ..., -0.0875361 ,\n",
      "       -0.01442257, -0.04152263], dtype=float32), array([[-0.0625548 ,  0.19905189, -0.07901664, ...,  0.19583888,\n",
      "        -0.12815107,  0.16648157],\n",
      "       [ 0.11868445,  0.04569052, -0.10624869, ..., -0.17127948,\n",
      "         0.01623619, -0.1627695 ],\n",
      "       [ 0.22910286, -0.00037863, -0.01343717, ..., -0.08216923,\n",
      "        -0.10621242, -0.04314717],\n",
      "       ...,\n",
      "       [-0.09008572,  0.01341501, -0.10414638, ..., -0.06603157,\n",
      "         0.00639764,  0.05713161],\n",
      "       [ 0.06322117, -0.0448063 ,  0.07710439, ...,  0.03892321,\n",
      "        -0.0655135 ,  0.05584122],\n",
      "       [ 0.29509076,  0.08169676, -0.07381504, ..., -0.05600933,\n",
      "         0.05988131, -0.09922221]], dtype=float32), array([-0.01038114, -0.0292908 ,  0.00017313, -0.06331998, -0.03651547,\n",
      "       -0.03194019,  0.04762529, -0.02447077], dtype=float32)]\n",
      "\n",
      "----- diversity: 0.9\n",
      "----- Generating with seed:\n",
      "0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 BAR 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 BAR 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b001000000 0b000000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed:\n",
      "0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 BAR 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 BAR 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b001000000 0b000000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed:\n",
      "0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 BAR 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 BAR 0b101000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 0b001000000 0b000000000 BAR 0b001000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b000000000 0b101000000 0b000000000 0b000000000 0b001000000 0b000000000 0b000000000 0b001000000 0b000000000 0b011000000 0b000000000 0b000000000 0b000000000 0b100000000 0b000000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! You might want to run main_post_process.py to get midi files. \n",
      "You need python-midi (https://github.com/vishnubob/python-midi) to run it.\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5cb1fbaf2c42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_weights.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
